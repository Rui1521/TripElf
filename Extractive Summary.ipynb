{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "import torch\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Help Functions\n",
    "stop_words = stopwords.words('English')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def is_eng(word):\n",
    "    for char in word:\n",
    "        if ord(char) < 32 or ord(char) > 126:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.replace('[^\\w\\s]', '')\n",
    "    text = text.replace('\\d+', '')\n",
    "    text = \" \".join([lemmatizer.lemmatize(word.lower()) for word in nltk.word_tokenize(text) if (word not in stop_words) \n",
    "                     and is_eng(word) and word.isalpha()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data and Merge Three Tables\n",
    "reviews =  pd.read_csv(\"reviews.csv\")\n",
    "listing = pd.read_csv(\"listings.csv\")\n",
    "neighbors = pd.read_csv(\"neighbourhoods.csv\")\n",
    "listing = listing.rename(columns={\"id\": \"listing_id\"})\n",
    "merged = pd.merge(listing, reviews, on = \"listing_id\", how =\"inner\")\n",
    "data = pd.merge(merged, neighbors, on = \"neighbourhood\", how =\"left\")[['neighbourhood','neighbourhood_group','comments']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL SUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = \" \".join(map(str, list(data[data[\"neighbourhood\"] == \"Harlem\"][\"comments\"])))\n",
    "raw_sent = sent_tokenize(comments)\n",
    "new_sent = [preprocess_text(raw_sent[i]) for i in range(len(raw_sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL(sen_words, doc_count):\n",
    "    length_sen = len(sen_words)\n",
    "    length_doc = sum(doc_count.values())\n",
    "    sen_count = Counter(sen_words)\n",
    "    \n",
    "    kl_score = 0\n",
    "    for item in sen_count.keys():\n",
    "        p = doc_count[item]/length_doc\n",
    "        q = sen_count[item]/length_sen\n",
    "        kl_score += p * np.log(p / q)\n",
    "    return kl_score\n",
    "\n",
    "def KLsum(orig, doc, L):\n",
    "    doc_words = [word for line in doc for word in line.split(\" \")]\n",
    "    doc_count = Counter(doc_words)\n",
    "    sentences = [line.split(\" \") for line in doc]\n",
    "    sen_score = [KL(sent,doc_count) for sent in sentences]\n",
    "    \n",
    "    pos = []\n",
    "    num_output = 0\n",
    "    l = 0\n",
    "    for i in np.argsort(sen_score):\n",
    "        l += len(orig[i].split(\" \"))\n",
    "        \n",
    "        pos.append(i)\n",
    "        if l > L:\n",
    "            break\n",
    "    pos = sorted(pos)\n",
    "    #print(pos)\n",
    "    summ = []\n",
    "    summ = [orig[i] for i in pos]\n",
    "    \n",
    "    return \". \".join(summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The apartment was very clean and everything was as advertised the location was great with plenty of shops and places to eat.We walked to central park in about 20 minutes and the subway was on the next corner up from the apartment all in all a great place to stay.We would stay there again.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KLsum(raw_sent, new_sent, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA SUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbors = list(set(data[data[\"neighbourhood_group\"] == \"Manhattan\"]['neighbourhood']))\n",
    "com_docs = []\n",
    "com_docs_orig = []\n",
    "for i in range(len(nbors)):\n",
    "    item = \" \".join(map(str, list(data[data[\"neighbourhood\"] == nbors[i]][\"comments\"])))\n",
    "    raw_sent = sent_tokenize(item)\n",
    "    new_sent = []\n",
    "    for i in range(len(raw_sent)-1, -1, -1):\n",
    "        wr = preprocess_text(raw_sent[i])\n",
    "        if wr != \"\":\n",
    "            new_sent.append(wr)\n",
    "        else:\n",
    "            del raw_sent[i]\n",
    "    com_docs_orig.append(\"%%%\".join(raw_sent))\n",
    "    com_docs.append(\"%%%\".join(new_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_nbors = TfidfVectorizer(sublinear_tf = True)\n",
    "duc_com = vectorizer_nbors.fit_transform(com_docs)\n",
    "word_com = {k:v for v,k in vectorizer_nbors.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_score(doc_prob, sent, vocabulary, components):\n",
    "    num_word = [vocabulary[w] for w in sent if w in vocabulary]\n",
    "    sen_prob = np.sum([components[:,num] for num in num_word], axis = 0)\n",
    "    \n",
    "    doc_pd = doc_prob / np.sum(doc_prob)\n",
    "    sen_pd = sen_prob / np.sum(sen_prob)\n",
    "\n",
    "    score = 0\n",
    "    try:\n",
    "        iter(sen_prob)\n",
    "    except TypeError:\n",
    "        res = 0\n",
    "    else:\n",
    "        for p, q in zip(doc_pd, sen_pd):\n",
    "            score += p*np.log(p/q)\n",
    "    #print(score)\n",
    "    return score\n",
    "\n",
    "def ldasum(doc_prob, orig, doc, vocabulary, components, L):\n",
    "    orig = orig.split(\"%%%\")\n",
    "    doc = doc.split(\"%%%\")\n",
    "    #print(len(orig), len(doc))\n",
    "    scores = [lda_score(doc_prob, sen.split(\" \"),vocabulary, components) for sen in doc if len(sen)!=1]\n",
    "    \n",
    "    pos = []\n",
    "    l = 0\n",
    "    for i in np.argsort(scores):\n",
    "        l += len(orig[i].split(\" \"))\n",
    "        pos.append(i)\n",
    "        #print(pos)\n",
    "        if l > L:\n",
    "            break\n",
    "    pos = sorted(pos)\n",
    "    \n",
    "    summ = []\n",
    "    summ = [orig[i] for i in pos]\n",
    "    \n",
    "    return \". \".join(summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=20, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_topic = 20\n",
    "lda_com = LDA(n_components=n_topic)\n",
    "lda_com.fit(duc_com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_sum_com = []\n",
    "for i in range(26):\n",
    "    if com_docs_orig[i] != \"\":\n",
    "        lda_sum_com.append(ldasum(lda_com.transform(duc_com)[i], com_docs_orig[i], com_docs[i], vectorizer_nbors.vocabulary_, lda_com.components_, 50))\n",
    "    else:\n",
    "        lda_sum_com.append(\"\")\n",
    "LDA_sum = {nbors[i]:lda_sum_com[i] for i in range(len(nbors))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Flatiron District', 'Battery Park City', \"Hell's Kitchen\", 'Harlem', 'West Village', 'East Village', 'Greenwich Village', 'Washington Heights', 'Chinatown', 'Civic Center', 'Nolita', 'Kips Bay', 'Marble Hill', 'Lower East Side', 'Midtown', 'Upper East Side', 'Murray Hill', 'Morningside Heights', 'Upper West Side', 'Financial District', 'Tribeca', 'Inwood', 'Chelsea', 'Little Italy', 'East Harlem', 'Roosevelt Island'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA_sum.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- We felt very secure in the neighborhood.. Everything was just as described.. Mimi provided towels and a box of toiletries for if you have forgotten anything.. I enjoyed my stay!. Bei Christopher und Co. Hat man wirklich eine super nette Truppe, die einen offen und sehr lieb empf√§ngt.. There is an elevator in the building so we had no issues getting our luggage in and out.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA_sum['East Harlem']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Password:\n"
     ]
    }
   ],
   "source": [
    "! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'allennlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6f196a0f913f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melmo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mElmoEmbedder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0melmo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElmoEmbedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'allennlp'"
     ]
    }
   ],
   "source": [
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "elmo = ElmoEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
